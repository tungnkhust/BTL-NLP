{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "allen_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JRx1zwkwSnp",
        "outputId": "f0b3e0c2-d10f-4610-e2cd-a3f54a1caa2f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82rTg9DwDGb",
        "outputId": "96eca9b8-bfde-4762-a10a-604cbc8d48d9"
      },
      "source": [
        "!pip install allennlp-models\n",
        "!pip install allennlp"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp-models in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.1)\n",
            "Requirement already satisfied: allennlp<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.4.0)\n",
            "Requirement already satisfied: conllu==4.4 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (4.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.8.1+cu101)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (6.0.3)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (1.17.84)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (4.41.1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (8.7.0)\n",
            "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.9.1+cu101)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.1.95)\n",
            "Requirement already satisfied: transformers<4.6,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (4.5.1)\n",
            "Requirement already satisfied: wandb<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.10.31)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.0.9)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.5,>=2.4.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.5,>=2.4.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.5,>=2.4.0->allennlp-models) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.84 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.5,>=2.4.0->allennlp-models) (1.20.84)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (56.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (21.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.5,>=2.4.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.5,>=2.4.0->allennlp-models) (1.24.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp<2.5,>=2.4.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.8.1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.1.17)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.5.4)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.1.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (5.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (5.4.8)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp<2.5,>=2.4.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.5,>=2.4.0->allennlp-models) (1.5.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<4.6,>=4.1->allennlp<2.5,>=2.4.0->allennlp-models) (3.4.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp<2.5,>=2.4.0->allennlp-models) (4.0.0)\n",
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.95)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.8.1+cu101)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: wandb<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.31)\n",
            "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.9.1+cu101)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.17.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: transformers<4.6,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.5.1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.17.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (56.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (21.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.1.17)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.12.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.8.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.13)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.0.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.84 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (1.20.84)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.4.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub>=0.0.8->allennlp) (3.4.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6,>=4.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NxOS6LoxNcf"
      },
      "source": [
        "from typing import Dict, Iterable, List\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from allennlp.data import DatasetReader, Instance, Vocabulary, TextFieldTensors\n",
        "from allennlp.data.fields import LabelField, TextField\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer, CharacterTokenizer\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder\n",
        "from allennlp.modules.seq2vec_encoders import LstmSeq2VecEncoder, CnnEncoder\n",
        "from allennlp.nn import util\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.data_loaders import MultiProcessDataLoader, DataLoader\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt7IsEkg8Yxr"
      },
      "source": [
        "class SentimentDataReader(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        char_tokenizer: CharacterTokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        max_tokens: int = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self.char_tokenizer = char_tokenizer or CharacterTokenizer()\n",
        "        if token_indexers is None:\n",
        "            self.token_indexers = {\n",
        "                    \"tokens\": SingleIdTokenIndexer(namespace=\"tokens\"),\n",
        "                    \"token_characters\": TokenCharactersIndexer(namespace=\"token_characters\")\n",
        "                }\n",
        "        else:\n",
        "            self.token_indexers = token_indexers\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def text_to_instance(self, text, label):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if self.max_tokens:\n",
        "                tokens = tokens[: self.max_tokens]\n",
        "\n",
        "        text_field = TextField(tokens, self.token_indexers)\n",
        "        fields = {\n",
        "            \"tokens\": text_field\n",
        "        }\n",
        "        if label:\n",
        "            fields['label'] = LabelField(label)\n",
        "        return Instance(fields)\n",
        "\n",
        "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
        "        df = pd.read_csv(file_path)\n",
        "        for _, row in df.iterrows():\n",
        "            text = row['text']\n",
        "            sentiment = row['topic']\n",
        "            yield self.text_to_instance(text, sentiment)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DncMFDfy7yua",
        "outputId": "4b2aee07-7dba-47e9-b680-f797c831f962"
      },
      "source": [
        "instances = SentimentDataReader().read('/content/drive/MyDrive/BTL_NLP/data/val.csv')\n",
        "c = 0\n",
        "for i in instances:\n",
        "  print(i)\n",
        "  c+=1\n",
        "  if c> 5:\n",
        "    break"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance with fields:\n",
            " \t tokens: TextField of length 6 with text: \n",
            " \t\t[giáo, trình, chưa, cụ, thể, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer', 'token_characters': 'TokenCharactersIndexer'} \n",
            " \t label: LabelField with label: program in namespace: 'labels'. \n",
            "\n",
            "Instance with fields:\n",
            " \t tokens: TextField of length 4 with text: \n",
            " \t\t[giảng, buồn, ngủ, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer', 'token_characters': 'TokenCharactersIndexer'} \n",
            " \t label: LabelField with label: lecturer in namespace: 'labels'. \n",
            "\n",
            "Instance with fields:\n",
            " \t tokens: TextField of length 8 with text: \n",
            " \t\t[giáo, viên, vui, tính, ,, tận, tâm, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer', 'token_characters': 'TokenCharactersIndexer'} \n",
            " \t label: LabelField with label: lecturer in namespace: 'labels'. \n",
            "\n",
            "Instance with fields:\n",
            " \t tokens: TextField of length 37 with text: \n",
            " \t\t[giảng, viên, nên, giao, bài, tập, nhiều, hơn, ,, chia, nhóm, để, làm, bài, tập, ,, giảng, kỹ,\n",
            "\t\tnhững, vấn, đề, trọng, tâm, của, môn, học, ,, đưa, ra, phương, pháp, để, học, hiệu, quả, hơn, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer', 'token_characters': 'TokenCharactersIndexer'} \n",
            " \t label: LabelField with label: lecturer in namespace: 'labels'. \n",
            "\n",
            "Instance with fields:\n",
            " \t tokens: TextField of length 25 with text: \n",
            " \t\t[giảng, viên, cần, giảng, bài, chi, tiết, hơn, ,, đi, sâu, hơn, code, và, chạy, thử, chương, trình,\n",
            "\t\tcó, trong, bài, giảng, nếu, được, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer', 'token_characters': 'TokenCharactersIndexer'} \n",
            " \t label: LabelField with label: lecturer in namespace: 'labels'. \n",
            "\n",
            "Instance with fields:\n",
            " \t tokens: TextField of length 18 with text: \n",
            " \t\t[nên, có, giảng, viên, nước, ngoài, dạy, để, sinh, viên, có, cơ, hội, thực, hành, giao, tiếp, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer', 'token_characters': 'TokenCharactersIndexer'} \n",
            " \t label: LabelField with label: lecturer in namespace: 'labels'. \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:60: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9rPDsq--_i3"
      },
      "source": [
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "\n",
        "class SimpleClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 vocab: Vocabulary,\n",
        "                 embedder: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder):\n",
        "        super().__init__(vocab)\n",
        "        self.embedder = embedder\n",
        "        self.encoder = encoder\n",
        "        num_labels = vocab.get_vocab_size(\"labels\")\n",
        "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "        self.f1_measure = F1Measure(1)\n",
        "\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
        "        embedded_text = self.embedder(tokens)\n",
        "        # Shape: (batch_size, num_tokens)\n",
        "        mask = util.get_text_field_mask(tokens)\n",
        "        # Shape: (batch_size, encoding_dim)\n",
        "        encoded_text = self.encoder(embedded_text, mask)\n",
        "        # Shape: (batch_size, num_labels)\n",
        "        logits = self.classifier(encoded_text)\n",
        "        # Shape: (batch_size, num_labels)\n",
        "        probs = torch.nn.functional.softmax(logits)\n",
        "        # Shape: (1,)\n",
        "        output = {\n",
        "            'probs': probs,\n",
        "            'logits': logits\n",
        "        }\n",
        "        if label is not None:\n",
        "            loss = torch.nn.functional.cross_entropy(logits, label)\n",
        "            output['loss'] = loss\n",
        "            self.accuracy(logits, label)\n",
        "            self.f1_measure(logits, label)\n",
        "\n",
        "        return output\n",
        "    \n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        metrics_result = self.f1_measure.get_metric(reset)\n",
        "        metrics_result['accuracy'] = self.accuracy.get_metric(reset)\n",
        "        \n",
        "        return metrics_result"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnw_q1GADuzI"
      },
      "source": [
        "import tempfile\n",
        "from typing import Dict, Iterable, List, Tuple\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import allennlp\n",
        "import torch\n",
        "from allennlp.data import (\n",
        "    DataLoader,\n",
        "    DatasetReader,\n",
        "    Instance,\n",
        "    Vocabulary,\n",
        "    TextFieldTensors,\n",
        ")\n",
        "from allennlp.data.data_loaders import SimpleDataLoader\n",
        "from allennlp.data.fields import LabelField, TextField\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
        "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.nn import util\n",
        "from allennlp.training.trainer import GradientDescentTrainer, Trainer\n",
        "from allennlp.training.optimizers import AdamOptimizer\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "\n",
        "\n",
        "train_path = '/content/drive/MyDrive/BTL_NLP/data/train.csv'\n",
        "test_path = '/content/drive/MyDrive/BTL_NLP/data/test.csv'\n",
        "val_path = '/content/drive/MyDrive/BTL_NLP/data/val.csv'\n",
        "\n",
        "def build_model(\n",
        "    vocab,\n",
        "    embedding_dim=100,\n",
        "    hidden_size=128,\n",
        "    char_embedding_dim=34,\n",
        "    dropout=0.4,\n",
        "    bidirectional=True,\n",
        "    word_embedding_pretrain_file=None\n",
        "):\n",
        "     # token embedding\n",
        "    embedding = Embedding(embedding_dim=embedding_dim, vocab_namespace='tokens', vocab=vocab, pretrained_file=word_embedding_pretrain_file)\n",
        "        \n",
        "    # char embedding with cnnencoder\n",
        "    character_embedding = Embedding(embedding_dim=char_embedding_dim, vocab_namespace='token_characters', vocab=vocab)\n",
        "    cnn_encoder = CnnEncoder(embedding_dim=char_embedding_dim, num_filters=char_embedding_dim, ngram_filter_sizes=(3,))\n",
        "    token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)\n",
        "\n",
        "    embedder = BasicTextFieldEmbedder(\n",
        "        {\n",
        "            \"tokens\": embedding,\n",
        "            \"token_characters\": token_encoder\n",
        "        }\n",
        "    )\n",
        "        \n",
        "    encoder = LstmSeq2VecEncoder(input_size=embedder.get_output_dim(), hidden_size=hidden_size, num_layers=2, bidirectional=bidirectional, dropout=dropout)\n",
        "\n",
        "    model = SimpleClassifier(\n",
        "        vocab,\n",
        "        embedder,\n",
        "        encoder\n",
        "    )\n",
        "    model.to('cuda')\n",
        "    return model\n",
        "\n",
        "def build_dataset_reader(max_tokens=80) -> DatasetReader:\n",
        "    return SentimentDataReader(max_tokens=max_tokens)\n",
        "\n",
        "\n",
        "def read_data(reader: DatasetReader, train_path, val_path) -> Tuple[List[Instance], List[Instance]]:\n",
        "    print(\"Reading data\")\n",
        "    training_data = list(reader.read(train_path))\n",
        "    validation_data = list(reader.read(val_path))\n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
        "    print(\"Building the vocabulary\")\n",
        "    return Vocabulary.from_instances(instances)\n",
        "\n",
        "def build_data_loaders(\n",
        "    train_data: List[Instance],\n",
        "    dev_data: List[Instance],\n",
        "    batch_size=64\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    train_loader = SimpleDataLoader(train_data, batch_size, shuffle=True)\n",
        "    dev_loader = SimpleDataLoader(dev_data, batch_size, shuffle=False)\n",
        "    return train_loader, dev_loader\n",
        "\n",
        "def build_trainer(\n",
        "    model,\n",
        "    serialization_dir,\n",
        "    train_loader,\n",
        "    dev_loader,\n",
        "    num_epochs=7,\n",
        "    grad_clipping=5,\n",
        "    weight_decay=0.0001\n",
        "):\n",
        "    parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
        "    optimizer = AdamOptimizer(parameters, lr=0.001, weight_decay=weight_decay)  # type: ignore\n",
        "    trainer = GradientDescentTrainer(\n",
        "        model=model,\n",
        "        serialization_dir=serialization_dir,\n",
        "        data_loader=train_loader,\n",
        "        validation_data_loader=dev_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        optimizer=optimizer,\n",
        "        grad_clipping=grad_clipping,\n",
        "        cuda_device=0\n",
        "\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "def run_training_loop(\n",
        "    serialization_dir='models',\n",
        "    checkpoint=False,\n",
        "    embedding_dim=100,\n",
        "    hidden_size=128,\n",
        "    char_embedding_dim=34,\n",
        "    dropout=0.4,\n",
        "    bidirectional=True,\n",
        "    word_embedding_pretrain_file=None,\n",
        "    num_epochs=7,\n",
        "    grad_clipping=5,\n",
        "    weight_decay=0.0001\n",
        "):    \n",
        "\n",
        "    print('num_epochs', num_epochs)\n",
        "    print('word_embedding_pretrain_file', word_embedding_pretrain_file)\n",
        "    print('dropout', dropout)\n",
        "    print('grad_clipping', grad_clipping)\n",
        "    print('weight_decay', weight_decay)\n",
        "    if checkpoint is False:\n",
        "        if os.path.exists(serialization_dir):\n",
        "            shutil.rmtree(serialization_dir)\n",
        "\n",
        "    dataset_reader = build_dataset_reader(max_tokens=80)\n",
        "\n",
        "    train_data, dev_data = read_data(dataset_reader, train_path, val_path)\n",
        "\n",
        "    vocab = build_vocab(train_data + dev_data)\n",
        "\n",
        "    print(vocab)\n",
        "    model = build_model( \n",
        "        vocab,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        char_embedding_dim=char_embedding_dim,\n",
        "        dropout=dropout,\n",
        "        bidirectional=bidirectional,\n",
        "        word_embedding_pretrain_file=word_embedding_pretrain_file\n",
        "      )\n",
        "\n",
        "    print(model)\n",
        "    \n",
        "    train_loader, dev_loader = build_data_loaders(train_data, dev_data, batch_size=128)\n",
        "    train_loader.index_with(vocab)\n",
        "    dev_loader.index_with(vocab)\n",
        "\n",
        "    trainer = build_trainer(model,\n",
        "                            serialization_dir, \n",
        "                            train_loader, \n",
        "                            dev_loader,\n",
        "                            num_epochs=num_epochs,\n",
        "                            grad_clipping=grad_clipping,\n",
        "                            weight_decay=weight_decay\n",
        "                            )\n",
        "\n",
        "  \n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "    return model, dataset_reader, vocab"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brl1Ws8BHneF",
        "outputId": "423a28bb-b8dc-4b52-e6a6-8976bb87ae3b"
      },
      "source": [
        "model, dataset_reader, vocab = run_training_loop(\n",
        "    serialization_dir='models',\n",
        "    checkpoint=False,\n",
        "    bidirectional=True,\n",
        "    embedding_dim=100,\n",
        "    hidden_size=128,\n",
        "    char_embedding_dim=34,\n",
        "    dropout=0.4,\n",
        "    word_embedding_pretrain_file='/content/drive/MyDrive/BTL_NLP/pretrained/viki_adapt/viki_adapt_w2v.txt',\n",
        "    num_epochs=5,\n",
        "    grad_clipping=5,\n",
        "    weight_decay=0.0001\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_epochs 5\n",
            "word_embedding_pretrain_file /content/drive/MyDrive/BTL_NLP/pretrained/viki_adapt/viki_adapt_w2v.txt\n",
            "dropout 0.4\n",
            "grad_clipping 5\n",
            "weight_decay 0.0001\n",
            "Reading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:60: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
            "  UserWarning,\n",
            "building vocab:  11%|#         | 1376/13009 [00:00<00:00, 13752.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building the vocabulary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "building vocab: 100%|##########| 13009/13009 [00:00<00:00, 13667.23it/s]\n",
            "100%|##########| 20577/20577 [00:00<00:00, 145771.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary with namespaces:\n",
            " \tNon Padded Namespaces: {'*tags', '*labels'}\n",
            " \tNamespace: tokens, Size: 2620 \n",
            " \tNamespace: token_characters, Size: 120 \n",
            " \tNamespace: labels, Size: 4 \n",
            "\n",
            "SimpleClassifier(\n",
            "  (embedder): BasicTextFieldEmbedder(\n",
            "    (token_embedder_tokens): Embedding()\n",
            "    (token_embedder_token_characters): TokenCharactersEncoder(\n",
            "      (_embedding): TimeDistributed(\n",
            "        (_module): Embedding()\n",
            "      )\n",
            "      (_encoder): TimeDistributed(\n",
            "        (_module): CnnEncoder(\n",
            "          (_activation): ReLU()\n",
            "          (conv_layer_0): Conv1d(34, 34, kernel_size=(3,), stride=(1,))\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): LstmSeq2VecEncoder(\n",
            "    (_module): LSTM(134, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "  0%|          | 0/90 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "precision: 0.6386, recall: 0.6206, f1: 0.6295, accuracy: 0.8145, batch_loss: 0.4253, loss: 0.4960 ||: 100%|##########| 90/90 [00:05<00:00, 16.80it/s]\n",
            "precision: 0.6894, recall: 0.7566, f1: 0.7214, accuracy: 0.8629, batch_loss: 0.2760, loss: 0.3455 ||: 100%|##########| 13/13 [00:00<00:00, 52.28it/s]\n",
            "precision: 0.7677, recall: 0.7974, f1: 0.7823, accuracy: 0.8879, batch_loss: 0.3297, loss: 0.3113 ||: 100%|##########| 90/90 [00:05<00:00, 17.10it/s]\n",
            "precision: 0.6706, recall: 0.8464, f1: 0.7483, accuracy: 0.8718, batch_loss: 0.2953, loss: 0.3497 ||: 100%|##########| 13/13 [00:00<00:00, 53.75it/s]\n",
            "precision: 0.8085, recall: 0.8305, f1: 0.8194, accuracy: 0.9075, batch_loss: 0.1998, loss: 0.2611 ||: 100%|##########| 90/90 [00:05<00:00, 17.19it/s]\n",
            "precision: 0.7560, recall: 0.7079, f1: 0.7311, accuracy: 0.8749, batch_loss: 0.2414, loss: 0.3355 ||: 100%|##########| 13/13 [00:00<00:00, 53.83it/s]\n",
            "precision: 0.8414, recall: 0.8483, f1: 0.8448, accuracy: 0.9208, batch_loss: 0.4419, loss: 0.2259 ||: 100%|##########| 90/90 [00:05<00:00, 17.01it/s]\n",
            "precision: 0.7205, recall: 0.8015, f1: 0.7589, accuracy: 0.8806, batch_loss: 0.3233, loss: 0.3445 ||: 100%|##########| 13/13 [00:00<00:00, 50.66it/s]\n",
            "precision: 0.8669, recall: 0.8732, f1: 0.8701, accuracy: 0.9341, batch_loss: 0.1345, loss: 0.1855 ||: 100%|##########| 90/90 [00:05<00:00, 17.15it/s]\n",
            "precision: 0.7128, recall: 0.7715, f1: 0.7410, accuracy: 0.8699, batch_loss: 0.3122, loss: 0.3863 ||: 100%|##########| 13/13 [00:00<00:00, 52.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xQwLX1S3wRG",
        "outputId": "e69911c8-77a4-4da1-99b3-7b72d901cd38"
      },
      "source": [
        "from allennlp.training.util import evaluate\n",
        "\n",
        "test_data = list(dataset_reader.read(test_path))\n",
        "data_loader = SimpleDataLoader(test_data, batch_size=64)\n",
        "data_loader.index_with(model.vocab)\n",
        "vocab.save_to_files('/content/drive/MyDrive/BTL_NLP/topic/vocabulary')\n",
        "results = evaluate(model.to('cpu'), data_loader)\n",
        "print(results)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary serialization directory /content/drive/MyDrive/BTL_NLP/topic/vocabulary is not empty\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "precision: 0.77, recall: 0.68, f1: 0.72, accuracy: 0.87, loss: 0.34 ||: : 50it [00:03, 16.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'precision': 0.7652859687805176, 'recall': 0.6783216595649719, 'f1': 0.719184398651123, 'accuracy': 0.8711307643714467, 'loss': 0.3364033916592598}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL241UvsrKVx"
      },
      "source": [
        "!cp -rf models/best.th /content/drive/MyDrive/BTL_NLP/topic"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYSgQBvEf656"
      },
      "source": [
        "def predict(self, text: str):\n",
        "        tokens = [Token(token) for token in text.split(' ')]\n",
        "        text_field = TextField(tokens, self._token_indexers)\n",
        "        instance = Instance({\n",
        "            \"tokens\": text_field\n",
        "        })\n",
        "        output = model.forward_on_instance(instance)\n",
        "        y_prediction = np.argmax(output['probs'], axis=-1)\n",
        "        y_probs = np.max(output['probs'], axis=-1)\n",
        "        prediction = vocab.get_token_from_index(y_prediction, namespace='labels')\n",
        "        return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dog9uXcZ3mjr"
      },
      "source": [
        "texts = text_df['text'].tolist()\n",
        "predictions = [predict(text) for text in texts]\n",
        "text_df['sentiment_pred'] = predictions\n",
        "test_df_false = test_df[test_df['sentiment'] != test_df['sentiment_pred']]\n",
        "test_df_false.to_csv('/content/drive/MyDrive/BTL_NLP/false_sent_pred.csv', index=False)\n",
        "\n",
        "test_df_true = test_df[test_df['sentiment'] == test_df['sentiment_pred']]\n",
        "test_df_true.to_csv('/content/drive/MyDrive/BTL_NLP/true_sent_pred.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}